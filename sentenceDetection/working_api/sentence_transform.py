### pip install sentence_transformers
### try: pip install transformers

import nltk
# Uncomment below line if punkt or such error pops up
nltk.download('punkt',quiet=True)

from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import re

# Loading Bert model, it will download the model upon first execution
model = SentenceTransformer('distilbert-base-nli-mean-tokens')

def sen_transformation(text, scene_len):
    '''
    Method Params:
    text: The text script for which the scenes are to be created
    scene_len: string short for short scene and long for long scene

    Function returns: A list of textual data i.e., scenes
    '''

    # Creating list of sentences
    sen_list = nltk.tokenize.sent_tokenize(text)

    # Counting number of sentences
    sen_count = len(sen_list)

    # Calculating number of keywords to be generated by
    # estimating 3 keywords per sentence
    num_key = 3 * sen_count

    # Preprocessing original text data before generating keywords
    # Removing punctuation from string data and
    # converting the text into lowercase
    new_text = re.sub(r'[^\w\s]', '', text).lower()

    # Candidate Keywords/Keyphrases
    n_gram_range = (1, 1)
    stop_words = "english"

    # Extract candidate words/phrases
    count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([new_text])
    candidates = count.get_feature_names()

    # Embeddings
    doc_embedding = model.encode([text])
    candidate_embeddings = model.encode(candidates)

    # Cosine Similarity
    top_n = num_key
    distances = cosine_similarity(doc_embedding, candidate_embeddings)
    keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]

    # Setting number of keywords and threshold for number of words as per long or short scene
    # For short scene, 5 keywords are used and word threshold is set to 30
    # For long scene, 10 keywords are used and word threshold is set to 60
    num_key_scene = 5
    word_thresh = 30
    if (scene_len == "long"):
        num_key_scene = 10
        word_thresh = 60

    # Checking the sentences for the keywords
    count = 0
    scenes = []
    text_temp = ""
    for i in range(len(sen_list)):

        # If number of keywords or number of words exceed the threshold or its the last iteration
        # Append the text in the scene and set count as zero and empty temp_text
        if (count > num_key_scene or len(re.findall(r'\w+', text_temp)) >= word_thresh or len(
                re.findall(r'\w+', text_temp + sen_list[i])) >= word_thresh):
            scenes.append(text_temp)
            count = 0
            text_temp = ""

        # No sentence would be omitted
        if (sen_list[i] not in text_temp):
            text_temp += sen_list[i]

            # Checking if keyword is present in the sentence
        # If yes, then increasing the count of keywords encountered
        for j in range(len(keywords)):
            if ((keywords[j] in sen_list[i].lower()) and (count <= num_key_scene)):
                count += 1
            if (count > num_key_scene):
                break
        # If it is the last iteration i.e., last sentence in the script
        # Then, append the text in the scene
        if (i == (len(sen_list) - 1)):
            scenes.append(text_temp)

    return scenes
